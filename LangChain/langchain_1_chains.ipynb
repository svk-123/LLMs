{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "77b8a653-911f-4a4b-a908-26f9b7612f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ref: Langchain in your pocket, Mehul Gupta\n",
    "### requires: langchain==0.0.343 openai==1.3.6\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/vino/api_keys')\n",
    "from api_key import OPENAI_API_KEY\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d8fa3bb3-5eb6-400b-ba7f-2a64826ad31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "llm=OpenAI(model_name=\"gpt-3.5-turbo-instruct\",api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ab09f29d-7d4e-4259-a52e-b58bd0dfae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate.from_template(\"Preprocess the given text by following the given steps in sequence.\\\n",
    "Follow only those steps that have a yes against them. Remove Number:{number},\\\n",
    "Remove punctuations : {punc} ,Word stemming : {stem}.\\\n",
    "Output just the preprocessed text. Text : {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a874df63-d8ee-43da-a610-770433c3a664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['number', 'punc', 'stem', 'text'], template='Preprocess the given text by following the given steps in sequence.Follow only those steps that have a yes against them. Remove Number:{number},Remove punctuations : {punc} ,Word stemming : {stem}.Output just the preprocessed text. Text : {text}')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a95b7a6c-f57d-4af9-b0a4-cf6fc741d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm,prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "411ce3d9-de81-4832-b7d8-a3e6bf9ef87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hey I got out of in Swimming\n"
     ]
    }
   ],
   "source": [
    "print(chain.run({'text':'Hey!! I got 12 out of 20 in Swimming', 'number': 'yes', 'punc':'yes', 'stem':'no'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "60c76bdc-fb0c-4a1d-850a-aca3a9019dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\" Complete a {length} story using the given beginning.\\\n",
    "The genre should be {genre} and the story should have an apt ending. Beginning: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4c7c757e-1416-4413-bbcb-9cdd9ecb2a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm,prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "beb4355f-b141-4a2b-a0f6-7eb091d77a42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " named Max\n",
      "\n",
      "Once there was a coder named Max, who had a passion for creating the most advanced and realistic video games\n",
      " His love for coding had led him to spend countless hours in front of his computer, perfecting his skills and creating one successful game after another\n",
      "\n",
      "\n",
      "But one day, while working on his latest project, Max stumbled upon a strange and mysterious line of code\n",
      " It appeared out of nowhere, and he couldn't figure out where it came from\n",
      " Being the perfectionist that he was, Max couldn't let it go and decided to investigate further\n",
      "\n",
      "\n",
      "As he delved deeper into the code, he noticed that it was linked to a game he had never seen before\n",
      " The game was called \"The Coder's Nightmare\" and it had a dark and ominous feel to it\n",
      " Max couldn't resist the temptation to try it out and see what it was all about\n",
      "\n",
      "\n",
      "The moment he clicked on \"play,\" a shiver ran down his spine\n",
      " The game had a creepy atmosphere, with eerie music playing in the background\n",
      " The objective of the game was to escape a haunted mansion while being chased by a dark figure\n",
      "\n",
      "\n",
      "At first, Max thought it was just a cleverly designed game, but as he progressed through the levels, things took a sinister turn\n",
      " The\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(chain.run({'length':'short','genre':'horror','text':'once there was a coder'}).replace('\\n','.').split('.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7117b51a-a091-4d42-bdfb-e81e888129d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from langchain import HuggingFacePipeline, PromptTemplate, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a06b4adb-41c3-40f7-92f4-bbe2614d86e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load neo-gpt model\n",
    "model = \"EleutherAI/gpt-neo-125m\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ef0b3819-af52-41e2-ba70-c360af60fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "\"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "36a66a12-5f2a-4bab-96e2-684c0524f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=PromptTemplate.from_template(\"Tell me about {entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "29286a7a-7156-4393-a5b9-ef94ed3c5bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = HuggingFacePipeline(pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b5626ec5-89a8-415a-b7f1-c5c828f7b847",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm_model,prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7c3177ee-3674-4097-af12-4159f459cad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in your world? I'm a huge fan of the word 'humanity'\n",
      " But I'm not a big fan of humans\n",
      " I'm not a big fan of humans\n",
      "\n",
      "\n",
      "I like humans, I think, but I don't like humans\n",
      " I like humans, I think, but I don't like humans\n",
      " It's just like I have never heard of anything like human beings before\n",
      " It's just a bunch of things\n",
      " The fact is there are people who are good for nothing\n",
      " It's just like we have to be very careful and very careful of people\n",
      " It's like we can't be very careful\n",
      " It seems like we are the world's most careful people and that's a big problem\n",
      "\n",
      "\n",
      "And then you know what is that? I can't really think about it\n",
      " And I can't see how anyone else could, if I thought about it\n",
      " And I don't have a real problem with being a little bit careful when it comes to\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(chain.run('humans').replace('\\n','.').split('.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d84ab7f-fd91-42b0-97c1-3b254d1fb9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Need Huggingface API token\n",
    "# from langchain.llms import HuggingFaceHub\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "# prompt_template = \"\"\"name an {entity}. Output just the name\"\"\"\n",
    "# prompt = PromptTemplate.from_template(prompt_template)\n",
    " \n",
    "# huggingface_llm=HuggingFaceHub(repo_id=\"google/flan-t5-small\", model_kwargs={\"temperature\": 0})\n",
    "# chain = LLMChain(llm=huggingface_llm,prompt=prompt)\n",
    "\n",
    "# print(chain.run({'entity':'country'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "97d8d00a-cc78-494f-b3d2-5c29e5d44695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.schema.messages import HumanMessage, SystemMessage\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# messages = [\n",
    "\n",
    "#     SystemMessage(content=\"You're a helpful assistant\"),\n",
    "#     HumanMessage(content=\"What should we do to stop pollution?\"),]\n",
    "\n",
    "# llm = ChatOpenAI(openai_api_key='your API')\n",
    "# llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c4e2b93e-ef73-4ee4-8fc1-8dcd63c8dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### auto sql_chain\n",
    "\n",
    "# from langchain.chat_models import ChatOpenAI from langchain.chains import create_sql_query_chain from langchain.utilities import SQLDatabase\n",
    "# db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n",
    "# llm = ChatOpenAI()\n",
    "# chain = create_sql_query_chain(llm,db)\n",
    "# response=chain.invoke({\"question\":\"Get the name of the employee with the highest salary\"})\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8974875f-2b78-4bed-b50b-14dd875f3fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LCEL\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.prompts import ChatPromptTemplate\n",
    "# from langchain.schema import StrOutputParser\n",
    "\n",
    "# prompt=ChatPromptTemplate.from_template(\"What is the city {person} is from? Translate: {sentence} in the city native language\")\n",
    "# model = ChatOpenAI()\n",
    "# chain = prompt | model | StrOutputParser()\n",
    "# chain.invoke({'person': \"Virat Kohli\", 'sentence':'how are you?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "77fcd0d8-ba3b-4899-8f42-be133151e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### multiple chains using LCEL\n",
    "# from operator import itemgetter\n",
    "# from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.schema import StrOutputParser\n",
    "# prompt1=ChatPromptTemplate.from_template(\"Which country won the {game} world cup?\") prompt2= ChatPromptTemplate.from_template(\"Suggest the best {entity} from {country}\")\n",
    "# model = ChatOpenAI()\n",
    "# chain1 = prompt1 | model | StrOutputParser()\n",
    "# chain2 = (\n",
    "#     {\"country\": chain1, \"entity\": itemgetter(\"entity\")}\n",
    "\n",
    "#     | prompt2\n",
    "\n",
    "#     | model\n",
    "\n",
    "#     | StrOutputParser()\n",
    "\n",
    "# )\n",
    "# chain2.invoke({\"game\": \"football\", \"entity\": \"dish\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53254289-fef8-43a0-8d7b-431ce024e0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
